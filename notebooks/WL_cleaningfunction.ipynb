{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f4f737-e548-47fa-8c47-d43b1da7fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import IFrame, display\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c296c3ba-ec27-4880-ab15-e339abad93cf",
   "metadata": {},
   "source": [
    "# ðŸ§¹Cleaning cleaning functions\n",
    "\n",
    "The company name cleaning function I've been working with explodes to 30GB in memory. It seriously shouldn't. Worth a refactor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d56249c2-d971-4e83-8337-4a321a63a31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src import locations as loc\n",
    "from src.data import utils as du\n",
    "from src.data.star import Star\n",
    "from src.data.datasets import Dataset\n",
    "from src.data.probabilities import Probabilities\n",
    "from src.data.clusters import Clusters\n",
    "from src.link.splink_linker import SplinkLinker\n",
    "from src.config import link_pipeline, stopwords\n",
    "from src.features.clean_complex import clean_comp_names\n",
    "from src.features.clean_basic import clean_company_name, list_join_to_string\n",
    "\n",
    "import splink.duckdb.comparison_library as cl\n",
    "import splink.duckdb.comparison_template_library as ctl\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import duckdb\n",
    "\n",
    "dotenv_path = find_dotenv()\n",
    "load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1f48ba-909b-49ac-86d8-c737308192f8",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Grab some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50d0cbb8-2dce-473c-ac90-2a6911e55f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/company_matching/lib/python3.9/site-packages/pandas/io/sql.py:1410: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  meta = MetaData(self.connectable, schema=schema)\n"
     ]
    }
   ],
   "source": [
    "star = Star(\n",
    "    schema = os.getenv(\"SCHEMA\"),\n",
    "    table = os.getenv(\"STAR_TABLE\")\n",
    ")\n",
    "probabilities = Probabilities(\n",
    "    schema = os.getenv(\"SCHEMA\"),\n",
    "    table = os.getenv(\"PROBABILITIES_TABLE\"),\n",
    "    star = star\n",
    ")\n",
    "clusters = Clusters(\n",
    "    schema = os.getenv(\"SCHEMA\"),\n",
    "    table = os.getenv(\"CLUSTERS_TABLE\"),\n",
    "    star = star\n",
    ")\n",
    "cl_x_exp=SplinkLinker.load(\n",
    "    path=Path(loc.DATA_SUBDIR['raw'], 'ch_x_exp.pickle')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e70080f-3545-49d3-934b-56ce675e3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_x_exp = SplinkLinker(\n",
    "    dataset = Dataset(\n",
    "        star_id=54717,\n",
    "        star=star\n",
    "    ), \n",
    "    probabilities=probabilities, \n",
    "    clusters=clusters, \n",
    "    n=2\n",
    ")\n",
    "cl_x_exp.get_data(\n",
    "    cluster_select={\n",
    "        '\"companieshouse\".\"companies\"': [\n",
    "            \"company_name as company_name\",\n",
    "            \"postcode as postcode\"\n",
    "        ]\n",
    "    },\n",
    "    dim_select=[\n",
    "        \"id\",\n",
    "        \"company_name\",\n",
    "        \"postcode\"\n",
    "    ]\n",
    ")\n",
    "cl_x_exp.save(path=Path(loc.DATA_SUBDIR['raw'], 'ch_x_exp.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db892e-453a-4b5a-a22e-2bc24f83512e",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8de440e6-fc0e-4445-94b4-5cf06612e186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = cl_x_exp.dim_raw.sample(int(1e4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1a4d47a-20a0-417a-8666-ac25d9e86654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name_arr</th>\n",
       "      <th>id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>postcode</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>name_unusual_tokens</th>\n",
       "      <th>names_tokens_stopwords</th>\n",
       "      <th>name_unusual_tokens_first5</th>\n",
       "      <th>name_unusual_tokens_last5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[stittenham, racing]</td>\n",
       "      <td>2693608</td>\n",
       "      <td>STITTENHAM RACING</td>\n",
       "      <td>YO60 6TN</td>\n",
       "      <td>[limited, uk, company, international, group, o...</td>\n",
       "      <td>racing stittenham</td>\n",
       "      <td></td>\n",
       "      <td>racin</td>\n",
       "      <td>enham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[h, williamson, sons, scalloway, limited]</td>\n",
       "      <td>1142224</td>\n",
       "      <td>H WILLIAMSON &amp; SONS (SCALLOWAY) LTD</td>\n",
       "      <td>ZE1 0TR</td>\n",
       "      <td>[limited, uk, company, international, group, o...</td>\n",
       "      <td>h scalloway sons williamson</td>\n",
       "      <td>limited</td>\n",
       "      <td>h sca</td>\n",
       "      <td>amson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[aj, mutch, and, son, limited]</td>\n",
       "      <td>2311858</td>\n",
       "      <td>AJ MUTCH AND SON LTD</td>\n",
       "      <td>OX29 6TE</td>\n",
       "      <td>[limited, uk, company, international, group, o...</td>\n",
       "      <td>aj mutch son</td>\n",
       "      <td>limited and</td>\n",
       "      <td>aj mu</td>\n",
       "      <td>h son</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[prolink, gb, limited]</td>\n",
       "      <td>31282</td>\n",
       "      <td>PROLINK GB LTD</td>\n",
       "      <td>SL8 5YS</td>\n",
       "      <td>[limited, uk, company, international, group, o...</td>\n",
       "      <td>gb prolink</td>\n",
       "      <td>limited</td>\n",
       "      <td>gb pr</td>\n",
       "      <td>olink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[gogna, enterprise, limited]</td>\n",
       "      <td>1871654</td>\n",
       "      <td>GOGNA ENTERPRISE LTD</td>\n",
       "      <td>HP18 9FE</td>\n",
       "      <td>[limited, uk, company, international, group, o...</td>\n",
       "      <td>enterprise gogna</td>\n",
       "      <td>limited</td>\n",
       "      <td>enter</td>\n",
       "      <td>gogna</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            company_name_arr       id  \\\n",
       "0                       [stittenham, racing]  2693608   \n",
       "1  [h, williamson, sons, scalloway, limited]  1142224   \n",
       "2             [aj, mutch, and, son, limited]  2311858   \n",
       "3                     [prolink, gb, limited]    31282   \n",
       "4               [gogna, enterprise, limited]  1871654   \n",
       "\n",
       "                          company_name  postcode  \\\n",
       "0                    STITTENHAM RACING  YO60 6TN   \n",
       "1  H WILLIAMSON & SONS (SCALLOWAY) LTD   ZE1 0TR   \n",
       "2                 AJ MUTCH AND SON LTD  OX29 6TE   \n",
       "3                       PROLINK GB LTD   SL8 5YS   \n",
       "4                 GOGNA ENTERPRISE LTD  HP18 9FE   \n",
       "\n",
       "                                           stopwords  \\\n",
       "0  [limited, uk, company, international, group, o...   \n",
       "1  [limited, uk, company, international, group, o...   \n",
       "2  [limited, uk, company, international, group, o...   \n",
       "3  [limited, uk, company, international, group, o...   \n",
       "4  [limited, uk, company, international, group, o...   \n",
       "\n",
       "           name_unusual_tokens names_tokens_stopwords  \\\n",
       "0            racing stittenham                          \n",
       "1  h scalloway sons williamson                limited   \n",
       "2                 aj mutch son            limited and   \n",
       "3                   gb prolink                limited   \n",
       "4             enterprise gogna                limited   \n",
       "\n",
       "  name_unusual_tokens_first5 name_unusual_tokens_last5  \n",
       "0                      racin                     enham  \n",
       "1                      h sca                     amson  \n",
       "2                      aj mu                     h son  \n",
       "3                      gb pr                     olink  \n",
       "4                      enter                     gogna  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_comp_names(df, primary_col=\"company_name\", stopwords=stopwords).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be4c3e6-d3ce-4dd9-a310-6a04e6a8c40d",
   "metadata": {},
   "source": [
    "What does this function actually do?\n",
    "\n",
    "* Standard clean of company name, returns tokens in an array\n",
    "* Standard clean of an array of company's second names -- this as array of arrays, presumably\n",
    "* Removes stopwords from the cleaned names\n",
    "    * By joining in the stopwords to EVERY ROW\n",
    "* Adds lists of terms removed etc (with pandas functions)\n",
    "\n",
    "I think we can make it way more efficient by overwriting columns, keeping it in duckdb, and ditching columns that aren't needed in prod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82bbdc32-4615-462e-929d-2685b28488c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_df = duckdb.sql(\"\"\"\n",
    "    select\n",
    "        *,\n",
    "        [company_name, company_name] as secondary_names\n",
    "    from\n",
    "        df;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccd3d84a-8baf-4df3-87b4-ed79524cf5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_except(input_col_name, terms_to_remove):\n",
    "    return rf\"\"\"\n",
    "    array_filter(\n",
    "        {input_col_name},\n",
    "        x -> not array_contains({terms_to_remove}, x)\n",
    "    )\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbb37824-389b-4a7a-afb7-3ab99a83e913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comp_names(\n",
    "    df, primary_col: str, secondary_col: str = None, stopwords: str = stopwords\n",
    "):\n",
    "\n",
    "    unnest_sql = f\"\"\"\n",
    "        select\n",
    "            *\n",
    "            replace (unnest({secondary_col}) as {secondary_col})\n",
    "        from\n",
    "            df;\n",
    "    \"\"\"\n",
    "    clean_primary_sql = f\"\"\"\n",
    "        select\n",
    "            *\n",
    "            replace ({clean_company_name(primary_col)} as {primary_col})\n",
    "        from\n",
    "            df;\n",
    "    \"\"\"\n",
    "    clean_secondary_sql = f\"\"\"\n",
    "        select\n",
    "            *\n",
    "            replace ({clean_company_name(secondary_col)} as {secondary_col})\n",
    "        from\n",
    "            df;\n",
    "    \"\"\"\n",
    "    stopwords_primary_sql = f\"\"\"\n",
    "        select\n",
    "            *\n",
    "            replace (\n",
    "                {\n",
    "                    list_join_to_string(\n",
    "                        array_except(primary_col, stopwords)\n",
    "                    )\n",
    "                }\n",
    "                as {primary_col}\n",
    "            )\n",
    "        from\n",
    "            df;\n",
    "    \"\"\"\n",
    "    stopwords_secondary_sql = f\"\"\"\n",
    "        select\n",
    "            *\n",
    "            replace (\n",
    "                {\n",
    "                    list_join_to_string(\n",
    "                        array_except(secondary_col, stopwords)\n",
    "                    )\n",
    "                }\n",
    "                as {secondary_col}\n",
    "            )\n",
    "        from\n",
    "            df;\n",
    "    \"\"\"\n",
    "    renest_sql = f\"\"\"\n",
    "        select\n",
    "            *\n",
    "            replace (list({secondary_col}) as {secondary_col})\n",
    "        from\n",
    "            df\n",
    "        group by all;\n",
    "    \"\"\"\n",
    "    \n",
    "    if secondary_col is not None:\n",
    "        to_run = [\n",
    "            unnest_sql, \n",
    "            clean_secondary_sql, \n",
    "            stopwords_secondary_sql, \n",
    "            renest_sql,\n",
    "            clean_primary_sql,\n",
    "            stopwords_primary_sql\n",
    "        ]\n",
    "    else:\n",
    "        to_run = [\n",
    "            clean_primary_sql,\n",
    "            stopwords_primary_sql\n",
    "        ]\n",
    "\n",
    "    for sql in to_run:\n",
    "        df = duckdb.sql(sql)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d7c8a-0046-4193-b4ff-ab7339e4f7d3",
   "metadata": {},
   "source": [
    "TODO: this gives kernel death with no reason why. Look into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ec261-a445-4206-afa7-7cfa743eda97",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_comp_names(\n",
    "    df,\n",
    "    primary_col=\"company_name\",\n",
    "    secondary_col=None,\n",
    "    stopwords=stopwords\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a58ea0ec-1221-4a49-a8eb-3277906334db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_df2 = duckdb.sql(f\"\"\"\n",
    "    select\n",
    "        *\n",
    "        replace (unnest(secondary_names) as secondary_names)\n",
    "    from\n",
    "        sec_df;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ae7b6b6-e45b-4960-96d3-62cfa6dc2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_df3 = duckdb.sql(f\"\"\"\n",
    "    select\n",
    "        *\n",
    "        replace ({clean_company_name(\"secondary_names\")} as secondary_names)\n",
    "    from\n",
    "        sec_df2;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eec4d939-95ea-4ff8-a168-3a2ea5c1200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_df4 = duckdb.sql(f\"\"\"\n",
    "    select\n",
    "        *\n",
    "        replace (\n",
    "            {\n",
    "                list_join_to_string(\n",
    "                    array_except(\"secondary_names\", stopwords)\n",
    "                )\n",
    "            }\n",
    "            as secondary_names\n",
    "        )\n",
    "    from\n",
    "        sec_df3;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6891b461-4e94-4e3f-8341-89d89947b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_df5 = duckdb.sql(f\"\"\"\n",
    "    select\n",
    "        *\n",
    "        replace (list(secondary_names) as secondary_names)\n",
    "    from\n",
    "        sec_df4\n",
    "    group by all;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed4f1b9-08ab-4a89-9822-248578013efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_except(input_col_name, terms_to_remove):\n",
    "    return rf\"\"\"\n",
    "    array_filter(\n",
    "        {input_col_name},\n",
    "        x -> not array_contains({terms_to_remove}, x)\n",
    "    )\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5c7e93e-b0e4-414e-b502-0a92eea2a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = duckdb.sql(f\"\"\"\n",
    "    select\n",
    "        *\n",
    "        replace ({clean_company_name(\"company_name\")} as company_name)\n",
    "    from\n",
    "        df;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efe14155-f93c-460e-ae18-833ae7f2c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = duckdb.sql(f\"\"\"\n",
    "    select\n",
    "        *\n",
    "        replace (\n",
    "            {\n",
    "                list_join_to_string(\n",
    "                    array_except(\"company_name\", stopwords)\n",
    "                )\n",
    "            }\n",
    "            as company_name\n",
    "        )\n",
    "    from\n",
    "        df2;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dba1858-6155-4612-b53a-2629f5fb1eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚   id    â”‚                 company_name                 â”‚ postcode â”‚\n",
       "â”‚  int64  â”‚                   varchar                    â”‚ varchar  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ 2720694 â”‚ lloyd julian                                 â”‚ NR6 7GA  â”‚\n",
       "â”‚  647217 â”‚ niels larsen                                 â”‚ WF5 0HP  â”‚\n",
       "â”‚  505204 â”‚ churchill fire                               â”‚ EC2A 3QR â”‚\n",
       "â”‚  618395 â”‚ buzz pinky                                   â”‚ PO9 2NA  â”‚\n",
       "â”‚ 3361781 â”‚ t f tull                                     â”‚ WD18 8RH â”‚\n",
       "â”‚  650314 â”‚ vct                                          â”‚ GU24 8HU â”‚\n",
       "â”‚ 2310276 â”‚ showerdrape std                              â”‚ M17 1DB  â”‚\n",
       "â”‚  249534 â”‚ maquet                                       â”‚ NE35 9PZ â”‚\n",
       "â”‚ 2321202 â”‚ fiera capital iom                            â”‚ IM1 1EU  â”‚\n",
       "â”‚ 2893212 â”‚ nature s buddy                               â”‚ SW17 0QF â”‚\n",
       "â”‚    Â·    â”‚       Â·                                      â”‚    Â·     â”‚\n",
       "â”‚    Â·    â”‚       Â·                                      â”‚    Â·     â”‚\n",
       "â”‚    Â·    â”‚       Â·                                      â”‚    Â·     â”‚\n",
       "â”‚ 1957717 â”‚ base childrenswear                           â”‚ IG8 8HF  â”‚\n",
       "â”‚   50716 â”‚ rascal clothing                              â”‚ EN11 0BE â”‚\n",
       "â”‚ 2145000 â”‚ gw wines                                     â”‚ WA14 4QF â”‚\n",
       "â”‚ 1754977 â”‚ jyw                                          â”‚ TA2 7AS  â”‚\n",
       "â”‚  891327 â”‚ digital print                                â”‚ NN7 2EG  â”‚\n",
       "â”‚ 1624804 â”‚ lff scotland                                 â”‚ AB32 6JL â”‚\n",
       "â”‚ 2628894 â”‚ dandara iom holdings                         â”‚ IM2 2SA  â”‚\n",
       "â”‚ 1415928 â”‚ ecom                                         â”‚ BD10 9TQ â”‚\n",
       "â”‚ 1701108 â”‚ shen zhen shi lang ma ke ji you xian gong si â”‚ AB10 1ZP â”‚\n",
       "â”‚ 1109511 â”‚ millerbrown                                  â”‚ HD9 6EB  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ ? rows (>9999 rows, 20 shown)                           3 columns â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d9479-aac2-45ec-b710-dd5be1f0b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_clean_company_name = f\"\"\"\n",
    "    select\n",
    "        {clean_company_name(primary_col)} as company_name_arr,\n",
    "        {\n",
    "            f\"{clean_company_name(secondary_col)} as secondary_names_arr, \"\n",
    "            if secondary_col\n",
    "            else \"\"\n",
    "        }\n",
    "        *\n",
    "    from df\n",
    "\"\"\"\n",
    "names_cleaned = duckdb.sql(sql_clean_company_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4157cf-ab24-4d6e-85e9-3c7de3eb4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"function\": clean_comp_names,\n",
    "\"arguments\": {\n",
    "    \"primary_col\": \"company_name\",\n",
    "    \"secondary_col\": None,\n",
    "    \"stopwords\": stopwords,\n",
    "},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c80dbd5-f446-4ce3-8196-f012652edf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"function\": clean_comp_names,\n",
    "\"arguments\": {\n",
    "    \"primary_col\": \"company_name\",\n",
    "    \"secondary_col\": None,\n",
    "    \"stopwords\": stopwords,\n",
    "},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe4affc-4f6c-4660-81fd-540aacd15055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comp_names(\n",
    "    df, primary_col: str, secondary_col: str = None, stopwords: str = stopwords\n",
    "):\n",
    "    \"\"\"\n",
    "    Lower case, remove punctuation & tokenise the primary company name into an array.\n",
    "    Extract tokens into: 'unusual' and 'stopwords'. Dedupe. Sort alphabetically.\n",
    "    Untokenise the unusual words back to a string.\n",
    "\n",
    "    Args:\n",
    "        df: a dataframe\n",
    "        primary_col: a column containing the company's main name\n",
    "        secondary_col: a column containing an array of the company's\n",
    "            secondary names\n",
    "        stopwords: a list of stopwords to use for this clean\n",
    "    Returns:\n",
    "        dataframe: company number, 'unusual' tokens', most common 3 tokens,\n",
    "            most common 4 to 6 tokens, list of previous names of company, postcode.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Refactor the silly nested f-strings\n",
    "\n",
    "    # CLEAN and TOKENISE\n",
    "    # To a new dataframe\n",
    "    sql_clean_company_name = f\"\"\"\n",
    "    select\n",
    "        {clean_company_name(primary_col)} as company_name_arr,\n",
    "        {\n",
    "            f\"{clean_company_name(secondary_col)} as secondary_names_arr, \"\n",
    "            if secondary_col\n",
    "            else \"\"\n",
    "        }\n",
    "        *\n",
    "    from df\n",
    "    \"\"\"\n",
    "    names_cleaned = duckdb.sql(sql_clean_company_name)  # noqa:F841\n",
    "\n",
    "    # Define STOPWORDS\n",
    "    # And join them in\n",
    "    stopword_tokens = pd.DataFrame({\"token_array\": [stopwords]})  # noqa:F841\n",
    "    sql_companies_arr_with_top = \"\"\"\n",
    "    select\n",
    "        *,\n",
    "        (select * from stopword_tokens) as stopwords\n",
    "    from names_cleaned\n",
    "    \"\"\"\n",
    "    with_common_terms = duckdb.sql(sql_companies_arr_with_top)  # noqa:F841\n",
    "\n",
    "    # EXTRACT the UNUSUAL and STOPWORD tokens\n",
    "    # We want the weird stuff from company names\n",
    "    # TODO: leave name_unusual_tokens (and secondary...) as array & remove split() below\n",
    "    def secondary_name_unusual_tokens():\n",
    "        # DuckDB needs a refactor, sorry\n",
    "        return list_join_to_string(array_except(\"secondary_names_arr\", \"stopwords\"))\n",
    "\n",
    "    def cat_names_tokens_stopwords(primary_arr, secondary_arr, stopwords):\n",
    "        # DuckDB needs a refactor, sorry\n",
    "        # return array_intersect(\"secondary_names_arr\", \"stopwords\")\n",
    "        primary = rf\"{array_intersect(primary_arr, stopwords)}\"\n",
    "        secondary = rf\"{array_intersect(primary_arr, stopwords)}\"\n",
    "\n",
    "        if secondary_arr:\n",
    "            return rf\"\"\"\n",
    "                array_cat(\n",
    "                    {primary},\n",
    "                    {secondary}\n",
    "                )\n",
    "            \"\"\"\n",
    "        else:\n",
    "            return rf\"{primary}\"\n",
    "\n",
    "    sql_manipulate_arrays = f\"\"\"\n",
    "    select\n",
    "        *,\n",
    "        {\n",
    "            list_join_to_string(\n",
    "                array_except(\"company_name_arr\", \"stopwords\")\n",
    "            )\n",
    "        }\n",
    "            as name_unusual_tokens,\n",
    "        {\n",
    "            (\n",
    "                f\"{secondary_name_unusual_tokens()} \"\n",
    "                \"as secondary_name_unusual_tokens\"\n",
    "            )\n",
    "            if secondary_col\n",
    "            else \"\"\n",
    "        }\n",
    "        {\n",
    "            cat_names_tokens_stopwords(\n",
    "                \"company_name_arr\",\n",
    "                \"secondary_names_arr\",\n",
    "                stopwords\n",
    "            )\n",
    "        } as names_tokens_stopwords\n",
    "    from with_common_terms\n",
    "    \"\"\"\n",
    "    clean = duckdb.sql(sql_manipulate_arrays)\n",
    "\n",
    "    clean_df = clean.df()\n",
    "\n",
    "    # DEDUPE names_tokens_stopwords\n",
    "    clean_df[\"name_unusual_tokens\"] = clean_df.name_unusual_tokens.apply(\n",
    "        lambda x: \" \".join(sorted(set(x.split()))) if pd.notnull(x) else x\n",
    "    )\n",
    "    if secondary_col:\n",
    "        clean_df[\n",
    "            \"secondary_name_unusual_tokens\"\n",
    "        ] = clean_df.secondary_name_unusual_tokens.apply(\n",
    "            lambda x: \" \".join(sorted(set(x.split()))) if pd.notnull(x) else x\n",
    "        )\n",
    "\n",
    "    clean_df[\"names_tokens_stopwords\"] = clean_df.names_tokens_stopwords.apply(\n",
    "        lambda x: \" \".join(set(x))\n",
    "    )\n",
    "\n",
    "    # Get HEAD and TAIL characters\n",
    "    # For blocking rules\n",
    "    clean_df[\"name_unusual_tokens_first5\"] = clean_df.name_unusual_tokens.str[:5]\n",
    "    clean_df[\"name_unusual_tokens_last5\"] = clean_df.name_unusual_tokens.str[-5:]\n",
    "\n",
    "    return clean_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "company_matching",
   "language": "python",
   "name": "company_matching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
